## <div align="center">Introduction to Transformer and Sequence Models</div>
## History of Sequence Models
The history of the sequence models in machine learning and natural language processing (NLP) is a fascinating journey, evolving from early statistica methods to advanced deep learning architecture like Transformers. Here is a detailed look at this progression : 
### Statistical Methods for Sequence Modelling
- **[Markov Chains (1950s)](https://github.com/SHIVAMAMBAH/Transformer/blob/main/Module%2001/Markov%20Chains/README.md)** :
  - The early foundations of sequence modelling relied on probablistic approach like Markov Chains.
  - Markov Chains assume that the probability of the each item in a sequence depends only on the previous item.
  - Despite its simplicity, the Markov assumption helped develop many early NLP applications including simple text generation and part-of-speech tagging.
- **Hidden Markov Models (HMMs) (1960s-1970s)**:
  - HMMs intorduced a way to model hidden or unsolved states influencing observed data, marking an improvement over simple Markov chains
  - Widely applied in speech recognition and bioinformatics, HMMs allowed models to handle sequences with unobserved (latent) variables, making them more expressive.
  - HMMs were used for tasks like speech recognition, where phonemes (sound units) needed to be infered from a sequence of audio signals, and named entiry recognition (NER).
