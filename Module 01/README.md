## <div align="center">Introduction to Transformer and Sequence Models</div>
The history of the sequence models in machine learning and natural language processing (NLP) is a fascinating journey, evolving from early statistica methods to advanced deep learning architecture like Transformers. Here is a detailed look at this progression : 
### Statistical Methods for Sequence Modelling
- **[Markov Chains (1950s)](https://github.com/SHIVAMAMBAH/Transformer/Module-01/Markov-Chains/README.md)** :
  - The early foundations of sequence modelling relied on probablistic approach like Markov Chains.
  - Markov Chains assume that the probability of the each item in a sequence depends only on the previous item.
  - Despite its simplicity, the Markov assumption helped develop many early NLP applications including simple text generation and part-of-speech tagging.
