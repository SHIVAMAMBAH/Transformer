### Module 01 : Introduction to Transformer and Sequence Models
- History of Sequence Models
- RNNs and LSTMs : Limitations
- Introduction to Transformers : Why they were Needed
- The "Attention us All You Need" Paper (Vaswani et al.2017)
### Module 02 : Transformer Architecture Fundamentals
- Encoder - Decoder Architecture
- Self -Attention and Multi-Head Attention
- Positional Encoding and its Role in Sequence Data
- Layer Normalization, Residual Connections, and Feed-Forward Networks
### Module 03 : Deep Dive into Attention Mechanism
- Dot-Product Attention v/s Addtive Attention
- Self-Attention in Detail : Query, Key and Value
- Multi-Head Attention and its Benefits
