### Module 01 : {Introduction to Transformer and Sequence Models](https://github.com/SHIVAMAMBAH/Transformer/tree/main/Module%2001)
- History of Sequence Models
- RNNs and LSTMs : Limitations
- Introduction to Transformers : Why they were Needed
- The ["Attention us All You Need" Paper (Vaswani et al.2017)](https://arxiv.org/pdf/1706.03762)
### Module 02 : Transformer Architecture Fundamentals
- Encoder - Decoder Architecture
- Self -Attention and Multi-Head Attention
- Positional Encoding and its Role in Sequence Data
- Layer Normalization, Residual Connections, and Feed-Forward Networks
### Module 03 : Deep Dive into Attention Mechanism
- Dot-Product Attention v/s Addtive Attention
- Self-Attention in Detail : Query, Key and Value
- Multi-Head Attention and its Benefits
### Module 04 : The transformer Model in Practice
- Embeddings in Transformer (word, Positional)
- Batching and Masking Technique
- Training Transformer :  Optimizers and Hyperparameter Tuning
- Practical Implementation Tips
### Module 05 : Popular Transformer Architecture
- BERT, GPT and Transformer-XL
- Evolution to BERT Variants (RoBERTs, ALBERT and GPT variants)
- Specialized Transformers (T5, BART, Longformer. BigBird)
### Module 06 : Training and Fine-Tuning Transformer for NLP Tasks
- Transfer Learning in NLP
- Tokenization Techniques (WordPiece, SentencePiece)
- Common NLP Tasks : Classification, QA, Summarization, Tokenization
- Using Hugging Face's Transformers Library
### Module 07 : Advanced Application of Transformers
- Transformers in Computer Vision, Audio, and Multimodel Application
- Zero-Shot Learning and Prompt Engineering
- Recent Advances : Efficient Transformers, Sparse Attention, and Memory-Augmented Transformers
### Module 08 : Optimizing Transformer for Production
- Speeding Up Inference : Quantization, Distillation, and Pruning
- Memory Optimization Techniques for Large Models
- Deploying Transformemr with ONNX, TensorTX, and other tools
### Module 09 : The Future of Transformer
- Transformers beyond NLP : Vision, Reinforcement Learning and Biomedical Application
- Emerging Trends and NExt-generation Transformer
- Open Research Questions and the Future of AI with Transformers
